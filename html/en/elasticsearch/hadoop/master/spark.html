
<!DOCTYPE HTML>
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" /><title>Apache Spark support</title><meta name="generator" content="DocBook XSL Stylesheets V1.78.1" /><link rel="home" href="index.html" title="Elasticsearch for Apache Hadoop [master]" /><link rel="up" href="reference.html" title="Elasticsearch for Apache Hadoop" /><link rel="prev" href="pig.html" title="Apache Pig support" /><link rel="next" href="storm.html" title="Apache Storm support" />
  
 
 	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
 	
	<meta name="description" content="" />
	<meta name="keywords" content="" />



	<link rel="shortcut icon" href="/favicon.ico" type="image/x-icon">
	<link rel="icon" href="/favicon.ico" type="image/x-icon">
	<!-- For third-generation iPad with high-resolution Retina display: -->
	<link rel="apple-touch-icon-precomposed" sizes="64x64" href="/favicon_64x64_16bit.png">

	<!-- For iPhone with high-resolution Retina display: -->

	<link rel="apple-touch-icon-precomposed" sizes="32x32" href="/favicon_32x32.png">

	<!-- For first- and second-generation iPad: -->

	<link rel="apple-touch-icon-precomposed" sizes="16x16" href="/favicon_16x16.png">

	<link type="text/css" rel="stylesheet" href="/static/css/skel.css" />	
	<link type="text/css" rel="stylesheet" href="/static/css/style.css" />	
	<script src="/static/js/skel.min.js"></script>
	<script type="text/javascript">form_name = "common_layout"</script>
	<script src="/static/js/init.js"></script>
	<script src="/static/js/classie.js"></script>
	

	
	<link type="text/css" rel="stylesheet" href="/static/css/prettify.css" />	
	<script type="text/javascript" src="/static/js/prettify.js"></script>



<link rel="stylesheet" type="text/css" href="styles.css" />

</head>
<body >
	
	<div class="header-cont">
		<div id="header-wrapper">
			

<div class="container">
	<div class="row">
		<div class="12u">
			<section>
				<div class="row">
					<div class="8u 6u(small) 6u(xsmall)">
						<div id="elastic">
							<h1><a href="/"></a></h1>
						</div>
						<!-- Searchbar Input box -->
						<form method="get" action="/search" name="searchForm" class="header-search-form">
						   <input type="text" id="search-header-autocomplete" name="q" class="form-control global-input" placeholder="elastic{search}">
						</form>
					</div>
					<div class="4u 6u(small) 6u(xsmall)">
						<!-- Mobile Menu -->
						<div class="mobile-menu-wrapper">
							<ul class="m-shortcuts">
								<li><a id="header-search" class="m-search" href="#"></a></li>
								<li><a class="m-guide" href="/guide"></a></li>
							</ul>											
							<nav class="nav-menu">
								<div class="menu-button">
									<span></span>
								</div>
								<div class="nav-menu-list-wrap">
									<div class="nav-menu-list">
										<h3><a href="/products">Products</a></h3>
										<ul>
											<li><a href="/products/elasticsearch">Elasticsearch</a></li>
											<li><a href="/products/logstash">Logstash</a></li>
											<li><a href="/products/kibana">Kibana</a></li>
											<li><a href="/products/shield">Shield</a></li>
											<li><a href="/products/marvel">Marvel</a></li>
											<li><a href="/products/hadoop">Hadoop</a></li>
											<li><a href="/subscriptions">Support</a></li>
											<li><a href="/downloads">Downloads</a></li>
										</ul>
										<h3><a href="/subscriptions">Subscriptions</a></h3>
										<h3><a href="/learn">Learn</a></h3>
										<ul>
											<li><a href="/guide">Docs</a></li>
											<li><a href="/videos">Videos</a></li>
											<li><a href="http://purchases.elasticsearch.com" target="_blank">Training</a></li>
											<li><a href="/blog">Blog</a></li>
										</ul>
										<h3><a href="/community">Community</a></h3>
										<ul>
											<li><a href="/community/meetups">Meetups</a></li>
										</ul>
										<h3><a href="/use-cases">Use Cases</a></h3>
										<h3><a href="/about">About</a></h3>
										<ul>
											<li><a href="/about/leadership">Leadership</a></li>
											<li><a href="/about/board">Board of Directors</a></li>
											<li><a href="/about/careers">Careers</a></li>
											<li><a href="/about/partners">Partners</a></li>
											<li><a href="/about/press">Press</a></li>
										</ul>
									</div>
								</div>
							</nav>
						</div>
						<!-- Shortcuts -->
						<nav id="shortcuts">
							<ul class="links">
								
									<li class="sc-downloads grayscale"><a href="/downloads" id="header_downloads">downloads</a></li>
								
									<li class="sc-docs grayscale"><a href="/guide" id="header_guide">docs</a></li>
								
									<li class="sc-support grayscale"><a href="/subscriptions" id="header_subscriptions">support</a></li>
								
									<li class="sc-contact grayscale"><a href="/contact" id="header_contact">contact</a></li>
								
							</ul>
						</nav>
					</div>
				</div>
			</section>
			

<section class="desktop-main-nav">
	<div class="row">
		<div class="9u">
			<!-- Nav -->
			<nav id="nav">
				<ul>
					
					<li>
						<a href="/products" id="nav_products" >PRODUCTS</a>
						
						<ul class="dropdown">
							
								<li><a href="/products/elasticsearch" id="nav_elasticsearch" >elasticsearch</a></li>								
							
								<li><a href="/products/logstash" id="nav_logstash" >logstash</a></li>								
							
								<li><a href="/products/kibana" id="nav_kibana" >kibana</a></li>								
							
								<li><a href="/products/shield" id="nav_shield" >shield</a></li>								
							
								<li><a href="/products/marvel" id="nav_marvel" >marvel</a></li>								
							
								<li><a href="/products/hadoop" id="nav_hadoop" >hadoop</a></li>								
							
								<li><a href="/subscriptions" id="nav_support" >support</a></li>								
							
								<li><a href="/downloads" id="nav_downloads" >downloads</a></li>								
								
						</ul>							
					</li>
					
					<li>
						<a href="/subscriptions" id="nav_subscriptions" >SUBSCRIPTIONS</a>
						
						<ul class="dropdown">
								
						</ul>							
					</li>
					
					<li>
						<a href="/learn" id="nav_learn" >LEARN</a>
						
						<ul class="dropdown">
							
								<li><a href="/guide" id="nav_guide" >docs</a></li>								
							
								<li><a href="/videos" id="nav_videos" >videos</a></li>								
							
								<li><a href="http://purchases.elastic.co" id="nav_training" >training</a></li>								
							
								<li><a href="/blog" id="nav_learn_blog" >blog</a></li>								
								
						</ul>							
					</li>
					
					<li>
						<a href="/community" id="nav_community" >COMMUNITY</a>
						
						<ul class="dropdown">
							
								<li><a href="/community/meetups" id="nav_meetups" >meetups</a></li>								
								
						</ul>							
					</li>
					
					<li>
						<a href="/use-cases" id="nav_usecases" >USE CASES</a>
						
						<ul class="dropdown">
								
						</ul>							
					</li>
					
					<li>
						<a href="/blog" id="nav_blog" >BLOG</a>
						
						<ul class="dropdown">
								
						</ul>							
					</li>
					
					<li>
						<a href="/about" id="nav_about" >ABOUT</a>
						
						<ul class="dropdown">
							
								<li><a href="/about/leadership" id="nav_leadership" >leadership</a></li>								
							
								<li><a href="/about/board" id="nav_board" >board of directors</a></li>								
							
								<li><a href="/about/careers" id="nav_careers" >careers</a></li>								
							
								<li><a href="/about/partners" id="nav_partners" >partners</a></li>								
							
								<li><a href="/about/press" id="nav_press" >press</a></li>								
								
						</ul>							
					</li>
					
				</ul>
			</nav>
		</div>
		<!-- Search -->
		<div class="3u">
			<div id="searchbar">
				<form method="get" action="/search" name="searchForm">
					<input type="text" value="" name="q" placeholder="elastic{search}" id="autocomplete">
					<button class="button icon" onclick="searchForm.submit()">GO</button>
				</form>
			</div>
		</div>
	</div>
</section>

		</div>
	</div>
</div>

<style>	
	
		.sc-downloads{ background: url("/assets/blt0b75d0822b89593e/shortcuts.png") no-repeat;}
	
		.sc-docs{ background: url("/assets/blt0b75d0822b89593e/shortcuts.png") no-repeat;}
	
		.sc-support{ background: url("/assets/blt0b75d0822b89593e/shortcuts.png") no-repeat;}
	
		.sc-contact{ background: url("/assets/blt0b75d0822b89593e/shortcuts.png") no-repeat;}
		
</style>
		</div>
	</div>
  	<div id="content">
  		
  			
            <div id="pageheader">
    <div class="container">
        <header>
            <h1>Learn |</h1>
            <h2>Docs</h2>
        </header>
    </div>
</div>
<section id="guide">

            <!-- start body -->
<div class="breadcrumbs"><span class="breadcrumb-link"><a href="index.html">Elasticsearch for Apache Hadoop
      [master]
    </a></span> » <span class="breadcrumb-link"><a href="reference.html">Elasticsearch for Apache Hadoop </a></span> » <span class="breadcrumb-node">Apache Spark support</span></div><div class="navheader"><span class="prev"><a href="pig.html">
              « 
              Apache Pig support</a>
           
        </span><span class="next">
           
          <a href="storm.html">Apache Storm support
               »
            </a></span></div><div class="chapter"><div class="titlepage"><div><div><h2 class="title"><a id="spark"></a>Apache Spark support<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h2></div></div></div><div class="toc"><dl><dt><span class="chapter"><a href="doc-sections.html">Documentation sections</a></span></dt><dt><span class="chapter"><a href="features.html">Key features</a></span></dt><dt><span class="chapter"><a href="requirements.html">Requirements</a></span></dt><dt><span class="chapter"><a href="install.html">Installation</a></span></dt><dt><span class="chapter"><a href="arch.html">Architecture</a></span></dt><dt><span class="chapter"><a href="configuration.html">Configuration</a></span></dt><dt><span class="chapter"><a href="configuration-runtime.html">Hadoop runtime options</a></span></dt><dt><span class="chapter"><a href="logging.html">Logging</a></span></dt><dt><span class="chapter"><a href="mapreduce.html">Map/Reduce integration</a></span></dt><dt><span class="chapter"><a href="cascading.html">Cascading support</a></span></dt><dt><span class="chapter"><a href="hive.html">Apache Hive integration</a></span></dt><dt><span class="chapter"><a href="pig.html">Apache Pig support</a></span></dt><dt><span class="chapter"><a href="spark.html">Apache Spark support</a></span></dt><dt><span class="chapter"><a href="storm.html">Apache Storm support</a></span></dt><dt><span class="chapter"><a href="mapping.html">Mapping and Types</a></span></dt><dt><span class="chapter"><a href="metrics.html">Hadoop Metrics</a></span></dt><dt><span class="chapter"><a href="troubleshooting.html">Troubleshooting</a></span></dt></dl></div><div class="blockquote"><table border="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top"> </td><td width="80%" valign="top"><p><a class="ulink" href="http://spark.apache.org" target="_top">Apache Spark</a> is a fast and general-purpose cluster computing system. It provides high-level APIs in Java, Scala and Python, and an optimized engine that supports general execution graphs.</p></td><td width="10%" valign="top"> </td></tr><tr><td width="10%" valign="top"> </td><td colspan="2" align="right" valign="top">--<span class="attribution">
Spark website
</span></td></tr></table></div><p>Spark provides fast iterative/functional-like capabilities over large data sets, typically by <span class="emphasis"><em>caching</em></span> data in memory. As opposed to the rest of the libraries mentioned in this documentation, Apache Spark is computing framework that is not tied to Map/Reduce itself however it does integrate with Hadoop, mainly to HDFS.
elasticsearch-hadoop allows Elasticsearch to be used in Spark in two ways: through the dedicated support available since 2.1 or through the Map/Reduce bridge since 2.0</p><h3><a id="spark-installation"></a>Installation<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><p>Just like other libraries, elasticsearch-hadoop needs to be available in Spark’s classpath. As Spark has multiple deployment modes, this can translate to the target classpath, whether it is on only one node (as is the case with the local mode - which will be used through-out the documentation) or per-node depending on the desired infrastructure.</p><h3><a id="spark-native"></a>Native support<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>Added in 2.1.</p></div></div><p>elasticsearch-hadoop provides <span class="emphasis"><em>native</em></span> integration between Elasticsearch and Apache Spark, in the form of a <code class="literal">RDD</code> (Resilient Distributed Dataset) (or <span class="emphasis"><em>Pair</em></span> <code class="literal">RDD</code> to be precise) that can read data from Elasticsearch. The <code class="literal">RDD</code> is offered in two <span class="emphasis"><em>flavors</em></span>: one for Scala (which returns the data as <code class="literal">Tuple2</code> with Scala collections) and one for Java (which returns the data as <code class="literal">Tuple2</code> containing <code class="literal">java.util</code> collections).</p><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>Whenever possible, consider using the <span class="emphasis"><em>native</em></span> integration as it offers the best performance and maximum flexibility.</p></div></div><h4><a id="spark-native-cfg"></a>Configuration<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>To configure one, one can set the various properties described in the <a class="xref" href="configuration.html" title="Configuration"><em>Configuration</em></a> chapter through the <a class="ulink" href="http://spark.apache.org/docs/1.0.1/programming-guide.html#initializing-spark" target="_top"><code class="literal">SparkConf</code></a> object:</p><pre class="programlisting prettyprint lang-scala">import org.apache.spark.SparkConf

val conf = new SparkConf().setAppName(appName).setMaster(master)
conf.set("es.index.auto.create", "true")</pre><pre class="programlisting prettyprint lang-java">SparkConf conf = new SparkConf().setAppName(appName).setMaster(master);
conf.set("es.index.auto.create", "true");</pre><h4><a id="spark-write"></a>Writing data to Elasticsearch<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>With elasticsearch-hadoop, any <code class="literal">RDD</code> can be saved to Elasticsearch as long as its content can be translated into documents. In practice this means the <code class="literal">RDD</code> type needs to be a <code class="literal">Map</code> (whether a Scala or a Java one), a <a class="ulink" href="http://docs.oracle.com/javase/tutorial/javabeans/" target="_top"><code class="literal">JavaBean</code></a> or a Scala <a class="ulink" href="http://docs.scala-lang.org/tutorials/tour/case-classes.html" target="_top">case class</a>. When that is not the case, one can easily <span class="emphasis"><em>transform</em></span> the data
in Spark or plug-in their own customer <a class="link" href="configuration.html#configuration-serialization" title="Serializationedit"><code class="literal">ValueWriter</code></a>.</p><p><strong>Scala. </strong>When using Scala, simply import the <code class="literal">org.elasticsearch.spark</code> package which, through the <a class="ulink" href="http://www.artima.com/weblogs/viewpost.jsp?thread=179766" target="_top"><span class="emphasis"><em>pimp my library</em></span></a> pattern, enriches the  <span class="emphasis"><em>any</em></span> <code class="literal">RDD</code> API with <code class="literal">saveToEs</code> methods:</p><pre class="programlisting prettyprint lang-scala">import org.apache.spark.SparkContext    <a id="CO45-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.apache.spark.SparkContext._

import org.elasticsearch.spark._        <a id="CO45-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

...

val conf = ...
val sc = new SparkContext(conf)         <a id="CO45-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

val numbers = Map("one" -&gt; 1, "two" -&gt; 2, "three" -&gt; 3)
val airports = Map("arrival" -&gt; "Otopeni", "SFO" -&gt; "San Fran")

sc.makeRDD<a id="CO45-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>(Seq(numbers, airports)).saveToEs<a id="CO45-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>("spark/docs")</pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
start Spark through its Scala API
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">makeRDD</code> creates an ad-hoc <code class="literal">RDD</code> based on the collection specified; any other <code class="literal">RDD</code> (in Java or Scala) can be passed in
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO45-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
index the content (namely the two <span class="emphasis"><em>documents</em></span> (numbers and airports)) in Elasticsearch under <code class="literal">spark/docs</code>
</p></td></tr></table></div><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>Scala users might be tempted to use <code class="literal">Seq</code> and the <code class="literal">→</code> notation for declaring <span class="emphasis"><em>root</em></span> objects (that is the JSON document) instead of using a <code class="literal">Map</code>. While similar, the first notation results in slightly different types that cannot be matched to a JSON document: <code class="literal">Seq</code> is an order sequence (in other words a list) while <code class="literal">←</code> creates a <code class="literal">Tuple</code> which is more or less an ordered, fixed number of elements. As such, a list of lists cannot be used as a document since it cannot be mapped to a JSON object; however it can be used freely within one. Hence why in the example above <code class="literal">Map(k→v)</code> was used instead of <code class="literal">Seq(k→v)</code></p></div></div><p>As an alternative to the <span class="emphasis"><em>implicit</em></span> import above, one can elasticsearch-hadoop Spark support in Scala through <code class="literal">EsSpark</code> in <code class="literal">org.elasticsearch.spark.rdd</code> package which acts as an utility class allowing explicit method invocations. Additionally instead of <code class="literal">Map</code>s (which are convenient but require one mapping per instance due to their difference in structure), use a <span class="emphasis"><em>case class</em></span> :</p><pre class="programlisting prettyprint lang-scala">import org.apache.spark.SparkContext
import org.elasticsearch.spark.rdd.EsSpark                        <a id="CO46-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

// define a case class
case class Trip(departure: String, arrival: String)               <a id="CO46-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

val upcomingTrip = Trip("OTP", "SFO")
val lastWeekTrip = Trip("MUC", "OTP")

val +RDD+ = sc.makeRDD(Seq(upcomingTrip, lastWeekTrip))           <a id="CO46-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
EsSpark.saveToEs(rdd, "spark/docs")                               <a id="CO46-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">EsSpark</code> import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Define a case class named <code class="literal">Trip</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create an <code class="literal">RDD</code> around the <code class="literal">Trip</code> instances
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO46-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Index the <code class="literal">RDD</code> explicitly through <code class="literal">EsSpark</code>
</p></td></tr></table></div><p>For cases where the id (or other metadata fields like <code class="literal">ttl</code> or <code class="literal">timestamp</code>) of the document needs to be specified, one can do so by setting the appropriate <a class="link" href="configuration.html#cfg-mapping" title="Mappingedit">mapping</a> namely <code class="literal">es.mapping.id</code>. Following the previous example, to indicate to Elasticsearch to use the field <code class="literal">id</code> as the document id, update the <code class="literal">RDD</code> configuration (it is also possible to set the property on the <code class="literal">SparkConf</code> though due to its global effect it is discouraged):</p><pre class="programlisting prettyprint lang-scala">EsSpark.saveToEs(rdd, "spark/docs", Map("es.mapping.id" -&gt; "id"))</pre><p><strong>Java. </strong>Java users have a dedicated class that provides a similar functionality to <code class="literal">EsSpark</code>, namely <code class="literal">JavaEsSpark</code> in the <code class="literal">org.elasticsearch.hadoop.spark.rdd.api.java</code> (a package similar to Spark’s <a class="ulink" href="https://spark.apache.org/docs/1.0.1/api/java/index.html?org/apache/spark/api/java/package-summary.html" target="_top">Java API</a>):</p><pre class="programlisting prettyprint lang-java">import org.apache.spark.api.java.JavaSparkContext;                              <a id="CO47-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.SparkConf;

import org.elasticsearch.spark.rdd.java.api.JavaEsSpark;                        <a id="CO47-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
...

SparkConf conf = ...
JavaSparkContext jsc = new JavaSparkContext(conf);                              <a id="CO47-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

Map&lt;String, ?&gt; numbers = ImmutableMap.of("one", 1, "two", 2);                   <a id="CO47-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
Map&lt;String, ?&gt; airports = ImmutableMap.of("OTP", "Otopeni", "SFO", "San Fran");

JavaRDD&lt;Map&lt;String, ?&gt;&gt; javaRDD = jsc.parallelize(ImmutableList.of(doc1, doc2));<a id="CO47-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>
JavaEsSpark.saveToEs(javaRDD, "spark/docs");                                    <a id="CO47-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO47-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO47-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO47-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
start Spark through its Java API
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO47-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
to simplify the example, use <a class="ulink" href="https://code.google.com/p/guava-libraries/" target="_top">Guava</a>(a dependency of Spark) <code class="literal">Immutable</code>* methods for simple <code class="literal">Map</code>, <code class="literal">List</code> creation
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO47-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
create a simple <code class="literal">RDD</code> over the two collections; any other <code class="literal">RDD</code> (in Java or Scala) can be passed in
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO47-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
index the content (namely the two <span class="emphasis"><em>documents</em></span> (numbers and airports)) in Elasticsearch under <code class="literal">spark/docs</code>
</p></td></tr></table></div><p>The code can be further simplifies by using Java 5 <span class="emphasis"><em>static</em></span> imports. Additionally, the <code class="literal">Map</code> (who’s mapping is dynamic due to its <span class="emphasis"><em>loose</em></span> structure) can be replaced with a <code class="literal">JavaBean</code>:</p><pre class="programlisting prettyprint lang-java">public class TripBean implements Serializable {
   private String departure, arrival;

   public TripBean(String departure, String arrival) {
       setDeparture(departure);
       setArrival(arrival);
   }

   public TripBean() {}

   public String getDeparture() { return departure; }
   public String getArrival() { return arrival; }
   public void setDeparture(String dep) { departure = dep; }
   public void setArrival(String arr) { arrival = arr; }
}</pre><pre class="programlisting prettyprint lang-java">import static org.elasticsearch.spark.java.api.JavaEsSpark;                <a id="CO48-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
...

TripBean upcoming = new TripBean("OTP", "SFO");
TripBean lastWeek = new TripBean("MUC", "OTP");

JavaRDD&lt;TripBean&gt; javaRDD = jsc.parallelize(
                            ImmutableList.of(upcoming, lastWeek));        <a id="CO48-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
saveToEs(javaRDD, "spark/docs");                                          <a id="CO48-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO48-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
statically import <code class="literal">JavaEsSpark</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO48-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
define an <code class="literal">RDD</code> containing <code class="literal">TripBean</code> instances (<code class="literal">TripBean</code> is a <code class="literal">JavaBean</code>)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO48-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
call <code class="literal">saveToEs</code> method without having to type <code class="literal">JavaEsSpark</code> again
</p></td></tr></table></div><p>Setting the document id (or other metadata fields like <code class="literal">ttl</code> or <code class="literal">timestamp</code>) is similar to its Scala counterpart though potentially bit more verbose depending on whether you are using the JDK classes or some other utilities (like Guava):</p><pre class="programlisting prettyprint lang-java">JavaEsSpark.saveToEs(javaRDD, "spark/docs", ImmutableMap.of("es.mapping.id", "id"));</pre><h4><a id="spark-write-json"></a>Writing existing JSON to Elasticsearch<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>For cases where the data in the <code class="literal">RDD</code> is already in JSON, elasticsearch-hadoop allows direct indexing <span class="emphasis"><em>without</em></span> applying any transformation; the data is taken as is and sent directly to Elasticsearch. As such, in this case, elasticsearch-hadoop expects either an <code class="literal">RDD</code>
containing <code class="literal">String</code> or byte arrays (<code class="literal">byte[]</code>/<code class="literal">Array[Byte]</code>), assuming each entry represents a JSON document. If the <code class="literal">RDD</code> does not have the proper signature, the <code class="literal">saveJsonToEs</code> methods cannot be applied (in Scala they will not be available).</p><p><strong>Scala. </strong>
</p><pre class="programlisting prettyprint lang-scala">val json1 = "{\"reason\" : \"business\",\"airport\" : \"SFO\"}"      <a id="CO49-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
val json2 = "{\"participants\" : 5,\"airport\" : \"OTP\"}"

new SparkContext(conf).makeRDD(Seq(json1, json2))
                                   .saveJsonToEs("spark/json-trips") <a id="CO49-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span></pre><p>
</p><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO49-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
example of an entry within the <code class="literal">RDD</code> - the JSON is <span class="emphasis"><em>written</em></span> as is, without any transformation
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO49-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
index the JSON data through the dedicated <code class="literal">saveJsonToEs</code> method
</p></td></tr></table></div><p><strong>Java. </strong>
</p><pre class="programlisting prettyprint lang-java">String json1 = "{\"reason\" : \"business\",\"airport\" : \"SFO\"}";  <a id="CO50-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
String json2 = "{\"participants\" : 5,\"airport\" : \"OTP\"}";

JavaContextSpark jsc = ...
JavaRDD&lt;String&gt;<a id="CO50-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span> stringRDD = jsc.parallelize(ImmutableList.of(json1, json2));
JavaEsSpark.saveJsonToEs(stringRDD, "spark/json-trips");             <a id="CO50-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span></pre><p>
</p><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO50-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
example of an entry within the <code class="literal">RDD</code> - the JSON is <span class="emphasis"><em>written</em></span> as is, without any transformation
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO50-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
notice the <code class="literal">RDD&lt;String&gt;</code> signature
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO50-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
index the JSON data through the dedicated <code class="literal">saveJsonToEs</code> method
</p></td></tr></table></div><h4><a id="spark-write-dyn"></a>Writing to dynamic/multi-resources<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>For cases when the data being written to Elasticsearch needs to be indexed under different buckets (based on the data content) one can use the <code class="literal">es.resource.write</code> field which accepts pattern that are resolved from the document content, at runtime. Following the aforementioned <a class="link" href="configuration.html#cfg-multi-writes" title="Dynamic/multi resource writesedit">media example</a>, one could configure it as follows:</p><p><strong>Scala. </strong>
</p><pre class="programlisting prettyprint lang-scala">val game = Map("media_type"<a id="CO51-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>-&gt;"game","title" -&gt; "FF VI","year" -&gt; "1994")
val book = Map("media_type" -&gt; "book","title" -&gt; "Harry Potter","year" -&gt; "2010")
val cd = Map("media_type" -&gt; "music","title" -&gt; "Surfing With The Alien")

sc.makeRDD(Seq(game, book, cd)).saveToEs("my-collection/{media-type}")  <a id="CO51-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span></pre><p>
</p><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO51-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Document <span class="emphasis"><em>key</em></span> used for splitting the data. Any field can be declared (but make sure it is available in all documents)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO51-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Save each object based on its resource pattern, in this example based on <code class="literal">media_type</code>
</p></td></tr></table></div><p>For each document/object about to be written, elasticsearch-hadoop will extract the <code class="literal">media_type</code> field and use its value to determine the target resource.</p><p><strong>Java. </strong>As expected, things in Java are strikingly similar:</p><pre class="programlisting prettyprint lang-java">Map&lt;String, ?&gt; game =
  ImmutableMap.of("media_type", "game", "title", "FF VI", "year", "1994");
Map&lt;String, ?&gt; book = ...
Map&lt;String, ?&gt; cd = ...

JavaRDD&lt;Map&lt;String, ?&gt;&gt; javaRDD =
                jsc.parallelize(ImmutableList.of(game, book, cd));
saveToEs(javaRDD, "my-collection/{media-type}");  <a id="CO52-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO52-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Save each object based on its resource pattern, <code class="literal">media_type</code> in this example
</p></td></tr></table></div><h4><a id="spark-write-meta"></a>Handling document metadata<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>Elasticsearch allows each document to have its own <a class="ulink" href="http://www.elasticsearch.org/guide/en/elasticsearch/guide/current/_document_metadata.html" target="_top">metadata</a>. As explained above, through the various <a class="link" href="configuration.html#cfg-mapping" title="Mappingedit">mapping</a> options one can customize these parameters so that their values are extracted from their belonging document. Further more, one can even include/exclude what parts of the data are backed to Elasticsearch. In Spark, elasticsearch-hadoop extends this functionality allowing metadata to be supplied <span class="emphasis"><em>outside</em></span> the document itself through the use of <a class="ulink" href="http://spark.apache.org/docs/latest/programming-guide.html#working-with-key-value-pairs" target="_top"><span class="emphasis"><em>pair</em></span> <code class="literal">RDD</code>s</a>.
In other words, for <code class="literal">RDD</code>s containing a key-value tuple, the metadata can be extracted from the key and the value used as the document source.</p><p>The metadata is described through the <code class="literal">Metadata</code> Java <a class="ulink" href="http://docs.oracle.com/javase/tutorial/java/javaOO/enum.html" target="_top">enum</a> within <code class="literal">org.elasticsearch.spark.rdd</code> package which identifies its type - <code class="literal">id</code>, <code class="literal">ttl</code>, <code class="literal">version</code>, etc…
This sounds more complicated than it is, so let us see some examples.</p><p><strong>Scala. </strong>Pair <code class="literal">RDD</code>s, or simply put <code class="literal">RDD</code>s with the signature <code class="literal">RDD[(K,V)]</code> can take advantage of the <code class="literal">saveToEsWithMeta</code> methods that are available either through the <span class="emphasis"><em>implicit</em></span> import of <code class="literal">org.elasticsearch.spark</code> package or <code class="literal">EsSpark</code> object.
To manually specify the id for each document, simply pass in the <code class="literal">Object</code> (not of type <code class="literal">Map</code>) in your <code class="literal">RDD</code>:</p><pre class="programlisting prettyprint lang-scala">val otp = Map("iata" -&gt; "OTP", "name" -&gt; "Otopeni")
val muc = Map("iata" -&gt; "MUC", "name" -&gt; "Munich")
val sfo = Map("iata" -&gt; "SFO", "name" -&gt; "San Fran")

// instance of SparkContext
val sc = ...

val airportsRDD<a id="CO53-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span> = sc.makeRDD(Seq((1, otp), (2, muc), (3, sfo)))  <a id="CO53-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
pairRDD.saveToEsWithMeta<a id="CO53-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>(airportsRDD, "airports/2015")</pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO53-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">airportsRDD</code> is a <span class="emphasis"><em>key-value</em></span> pair <code class="literal">RDD</code>; it is created from a <code class="literal">Seq</code> of <code class="literal">tuple</code>s
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO53-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The key of each tuple within the <code class="literal">Seq</code> represents the <span class="emphasis"><em>id</em></span> of its associated value/document; in other words, document <code class="literal">otp</code> has id <code class="literal">1</code>, <code class="literal">muc</code> <code class="literal">2</code> and <code class="literal">sfo</code> <code class="literal">3</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO53-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
</p></td></tr></table></div><p>When more than just the id needs to be specified, one should use a <code class="literal">scala.collection.Map</code> with keys of type <code class="literal">org.elasticsearch.spark.rdd.Metadata</code>:</p><pre class="programlisting prettyprint lang-scala">import org.elasticsearch.spark.rdd.Metadata._          <a id="CO54-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

val otp = Map("iata" -&gt; "OTP", "name" -&gt; "Otopeni")
val muc = Map("iata" -&gt; "MUC", "name" -&gt; "Munich")
val sfo = Map("iata" -&gt; "SFO", "name" -&gt; "San Fran")

// metadata for each document
// note it's not required for them to have the same structure
val otpMeta = Map(ID -&gt; 1, TTL -&gt; "3h")                <a id="CO54-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
val mucMeta = Map(ID -&gt; 2, VERSION -&gt; "23")            <a id="CO54-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
val sfoMeta = Map(ID -&gt; 3)                             <a id="CO54-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>

// instance of SparkContext
val sc = ...

val airportsRDD = sc.makeRDD<a id="CO54-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span>(Seq((otpMeta, otp), (mucMeta, muc), (sfoMeta, sfo)))
pairRDD.saveToEsWithMeta(airportsRDD, "airports/2015") <a id="CO54-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Import the <code class="literal">Metadata</code> enum
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The metadata used for <code class="literal">otp</code> document. In this case, <code class="literal">ID</code> with a value of 1 and <code class="literal">TTL</code> with a value of <code class="literal">3h</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The metadata used for <code class="literal">muc</code> document. In this case, <code class="literal">ID</code> with a value of 2 and <code class="literal">VERSION</code> with a value of <code class="literal">23</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The metadata used for <code class="literal">sfo</code> document. In this case, <code class="literal">ID</code> with a value of 3
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The metadata and the documents are assembled into a <span class="emphasis"><em>pair</em></span> <code class="literal">RDD</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO54-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The <code class="literal">RDD</code> is saved accordingly using the <code class="literal">saveToEsWithMeta</code> method
</p></td></tr></table></div><p><strong>Java. </strong>In a similar fashion, on the Java side, <code class="literal">JavaEsSpark</code> provides <code class="literal">saveToEsWithMeta</code> methods that are applied to <code class="literal">JavaPairRDD</code> (the equivalent in Java of <code class="literal">RDD[(K,V)]</code>). Thus to save documents based on their ids one can use:</p><pre class="programlisting prettyprint lang-java">import org.elasticsearch.spark.java.api.JavaEsSpark;

// data to be saved
Map&lt;String, ?&gt; otp = ImmutableMap.of("iata", "OTP", "name", "Otopeni");
Map&lt;String, ?&gt; jfk = ImmutableMap.of("iata", "JFK", "name", "JFK NYC");

JavaSparkContext jsc = ...

// create a pair +RDD+ between the id and the docs
JavaPairRDD&lt;?, ?&gt; pairRdd = jsc.parallelizePairs<a id="CO55-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>(ImmutableList.of(
        new Tuple2&lt;Object, Object&gt;(1, otp),          <a id="CO55-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
        new Tuple2&lt;Object, Object&gt;(2, jfk)));        <a id="CO55-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
JavaEsSpark.saveToEsWithMeta(pairRDD, target);       <a id="CO55-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a <code class="literal">JavaPairRDD</code> by using Scala <code class="literal">Tuple2</code> class wrapped around the document id and the document itself
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Tuple for the first document wrapped around the id (<code class="literal">1</code>) and the doc (<code class="literal">otp</code>) itself
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Tuple for the second document wrapped around the id (<code class="literal">2</code>) and <code class="literal">jfk</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO55-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
The <code class="literal">JavaPairRDD</code> is saved accordingly using the keys as a id and the values as documents
</p></td></tr></table></div><p>When more than just the id needs to be specified, one can chose to use a <code class="literal">java.util.Map</code> populated with keys of type <code class="literal">org.elasticsearch.spark.rdd.Metadata</code>:</p><pre class="programlisting prettyprint lang-java">import org.elasticsearch.spark.java.api.JavaEsSpark;
import org.elasticsearch.spark.rdd.Metadata;          <a id="CO56-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

import static org.elasticsearch.spark.rdd.Metadata.*; <a id="CO56-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

// data to be saved
Map&lt;String, ?&gt; otp = ImmutableMap.of("iata", "OTP", "name", "Otopeni");
Map&lt;String, ?&gt; sfo = ImmutableMap.of("iata", "SFO", "name", "San Fran");

// metadata for each document
// note it's not required for them to have the same structure
Map&lt;Metadata, Object&gt; otpMeta<a id="CO56-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span> = ImmutableMap.&lt;Metadata, Object&gt;<a id="CO56-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span> of(ID, 1, TTL, "1d");
Map&lt;Metadata, Object&gt; sfoMeta<a id="CO56-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span> = ImmutableMap.&lt;Metadata, Object&gt; of(ID, "2", VERSION, "23");

JavaSparkContext jsc = ...

// create a pair +RDD+ between the id and the docs
JavaPairRDD&lt;?, ?&gt; pairRdd = jsc.parallelizePairs&lt;(ImmutableList.of(
        new Tuple2&lt;Object, Object&gt;(otpMeta, otp),    <a id="CO56-6"></a><span><img src="images/icons/callouts/6.png" alt="" /></span>
        new Tuple2&lt;Object, Object&gt;(sfoMeta, sfo)));  <a id="CO56-7"></a><span><img src="images/icons/callouts/7.png" alt="" /></span>
JavaEsSpark.saveToEsWithMeta(pairRDD, target);       <a id="CO56-8"></a><span><img src="images/icons/callouts/8.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">Metadata</code> <code class="literal">enum</code> describing the document metadata that can be declared
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
static import for the <code class="literal">enum</code> to refer to its values in short format (<code class="literal">ID</code>, <code class="literal">TTL</code>, etc…)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Metadata for <code class="literal">otp</code> document
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Boiler-plate construct for forcing the <code class="literal">of</code> method generic signature
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Metadata for <code class="literal">sfo</code> document
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-6"><span><img src="images/icons/callouts/6.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Tuple between <code class="literal">otp</code> (as the value) and its metadata (as the key)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-7"><span><img src="images/icons/callouts/7.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Tuple associating <code class="literal">sfo</code> and its metadata
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO56-8"><span><img src="images/icons/callouts/8.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">saveToEsWithMeta</code> invoked over the <code class="literal">JavaPairRDD</code> containing documents and their respective metadata
</p></td></tr></table></div><h4><a id="spark-read"></a>Reading data from Elasticsearch<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>For reading, one should define the Elasticsearch <code class="literal">RDD</code> that <span class="emphasis"><em>streams</em></span> data from Elasticsearch to Spark.</p><p><strong>Scala. </strong>Similar to writing, the <code class="literal">org.elasticsearch.spark</code> package, enriches the <code class="literal">SparkContext</code> API with <code class="literal">esRDD</code> methods:</p><pre class="programlisting prettyprint lang-scala">import org.apache.spark.SparkContext    <a id="CO57-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.apache.spark.SparkContext._

import org.elasticsearch.spark._        <a id="CO57-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

...

val conf = ...
val sc = new SparkContext(conf)         <a id="CO57-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

val +RDD+ = sc.esRDD("radio/artists")     <a id="CO57-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO57-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO57-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO57-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
start Spark through its Scala API
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO57-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
a dedicated <code class="literal">RDD</code> for Elasticsearch is created for index <code class="literal">radio/artists</code>
</p></td></tr></table></div><p>The method can be overloaded to specify an additional query or even a configuration <code class="literal">Map</code> (overriding <code class="literal">SparkConf</code>):</p><pre class="programlisting prettyprint lang-scala">...
import org.elasticsearch.spark._

...
val conf = ...
val sc = new SparkContext(conf)

sc.esRDD("radio/artists", "?me*") <a id="CO58-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO58-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
create an <code class="literal">RDD</code> streaming all the documents matching <code class="literal">me*</code> from index <code class="literal">radio/artists</code>
</p></td></tr></table></div><p>The documents from Elasticsearch are returned, by default, as a <code class="literal">Tuple2</code> containing as the first element the document id and the second element the actual document represented through Scala <a class="ulink" href="http://docs.scala-lang.org/overviews/collections/overview.html" target="_top">collections</a>, namely one `Map[String, Any]`where the keys represent the field names and the value their respective values.</p><p><strong>Java. </strong>Java users have a dedicated <code class="literal">JavaPairRDD</code> that works the same as its Scala counterpart however the returned <code class="literal">Tuple2</code> values (or second element) returns the documents as native, <code class="literal">java.util</code> collections.</p><pre class="programlisting prettyprint lang-java">import org.apache.spark.api.java.JavaSparkContext;               <a id="CO59-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.elasticsearch.spark.java.api.JavaEsSpark;             <a id="CO59-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
...

SparkConf conf = ...
JavaSparkContext jsc = new JavaSparkContext(conf);               <a id="CO59-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

JavaPairRDD&lt;String, Map&lt;String, Object&gt;&gt; esRDD =
                        JavaEsSpark.esRDD(jsc, "radio/artists"); <a id="CO59-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO59-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO59-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO59-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
start Spark through its Java API
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO59-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
a dedicated <code class="literal">JavaPairRDD</code> for Elasticsearch is created for index <code class="literal">radio/artists</code>
</p></td></tr></table></div><p>In a similar fashion one can use the overloaded <code class="literal">esRDD</code> methods to specify a query or pass a <code class="literal">Map</code> object for advanced configuration.
Let us see how this looks like, but this time around using <a class="ulink" href="http://docs.oracle.com/javase/1.5.0/docs/guide/language/static-import.html" target="_top">Java static imports</a> - further more, let us discard the documents ids and retrieve only the <code class="literal">RDD</code> values:</p><pre class="programlisting prettyprint lang-java">import static org.elasticsearch.spark.java.api.JavaEsSpark.*;          <a id="CO60-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

...
JavaRDD&lt;Map&lt;String, Object&gt;&gt; esRDD =
                        esRDD(jsc, "radio/artists", "?me*"<a id="CO60-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>).values()<a id="CO60-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>;</pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO60-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
statically import <code class="literal">JavaEsSpark</code> class
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO60-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
create an <code class="literal">RDD</code> streaming all the documents starting with <code class="literal">me</code> from index <code class="literal">radio/artists</code>. Note the method does not have to be fully qualified due to the static import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO60-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
return only <span class="emphasis"><em>values</em></span> of the <code class="literal">PairRDD</code> - hence why the result is of type <code class="literal">JavaRDD</code> and <span class="emphasis"><em>not</em></span> <code class="literal">JavaPairRDD</code>
</p></td></tr></table></div><p>By using the <code class="literal">JavaEsSpark</code> API, one gets a hold of Spark’s dedicated <code class="literal">JavaPairRDD</code> which are better suited in Java environments than the base <code class="literal">RDD</code> (due to its Scala
signatures). Moreover, the dedicated <code class="literal">RDD</code> returns Elasticsearch documents as proper Java collections so one does not have to deal with Scala collections (which
is typically the case with <code class="literal">RDD</code>s). This is particularly powerful when using Java 8, which we strongly recommend as its
<a class="ulink" href="http://docs.oracle.com/javase/tutorial/java/javaOO/lambdaexpressions.html" target="_top">lambda expressions</a> make collection processing <span class="emphasis"><em>extremely</em></span> concise.</p><p>To wit, let us assume one wants to filter the documents from the <code class="literal">RDD</code> and return only those that contain a value that contain <code class="literal">mega</code> (please ignore the fact one can and should do the filtering directly through Elasticsearch).</p><p>In versions prior to Java 8, the code would look something like this:</p><pre class="programlisting prettyprint lang-java">JavaRDD&lt;Map&lt;String, Object&gt;&gt; esRDD =
                        esRDD(jsc, "radio/artists", "?me*").values();
JavaRDD&lt;Map&lt;String, Object&gt;&gt; filtered = esRDD.filter(
    new Function&lt;Map&lt;String, Object&gt;, Boolean&gt;() {
      @Override
      public Boolean call(Map&lt;String, Object&gt; map) throws Exception {
          returns map.contains("mega");
      }
    });</pre><p>with Java 8, the filtering becomes a one liner:</p><pre class="programlisting prettyprint lang-java">JavaRDD&lt;Map&lt;String, Object&gt;&gt; esRDD =
                        esRDD(jsc, "radio/artists", "?me*").values();
JavaRDD&lt;Map&lt;String, Object&gt;&gt; filtered = esRDD.filter(doc -&gt;
                                                doc.contains("mega"));</pre><h3><a id="spark-sql"></a>Spark SQL support<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><div class="note admon"><div class="icon"><img alt="Note" src="images/icons/note.png" /></div><div class="admon_content"><p>Added in 2.1.</p></div></div><div class="blockquote"><table border="0" class="blockquote" summary="Block quote"><tr><td width="10%" valign="top"> </td><td width="80%" valign="top"><p><a class="ulink" href="http://spark.apache.org/sql/" target="_top">Spark SQL</a> allows relational queries expressed in SQL, HiveQL, or Scala to be executed using Spark. At the core of this component is a new type of <code class="literal">RDD</code>, SchemaRDD. A SchemaRDD is similar to a table in a traditional relational database. […] Spark SQL is currently an alpha component.</p></td><td width="10%" valign="top"> </td></tr><tr><td width="10%" valign="top"> </td><td colspan="2" align="right" valign="top">--<span class="attribution">
Spark website
</span></td></tr></table></div><p>On top of the core Spark support, elasticsearch-hadoop also provides integration with Spark SQL. In other words, Elasticsearch becomes a <span class="emphasis"><em>native</em></span> source for Spark SQL so that data can be indexed and queried from Spark SQL <span class="emphasis"><em>transparently</em></span>.</p><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>Spark SQL works with <span class="emphasis"><em>structured</em></span> data - in other words, all entries are expected to have the <span class="emphasis"><em>same</em></span> structure (same number of fields, of the same type and name). Using unstructured data (documents with different
structures) is <span class="emphasis"><em>not</em></span> supported and will cause problems. For such cases, use <code class="literal">PairRDD</code>s.</p></div></div><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>Spark SQL is a young component, going through significant changes between releases. While we strive to maintain compatibility going forward, the functionality between different Spark releases may change.</p></div></div><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>Spark SQL is a <span class="emphasis"><em>separate</em></span> component from Spark <span class="emphasis"><em>core</em></span>. To use it, make sure to properly add it to your classpath - please see the <a class="xref" href="requirements.html" title="Requirements"><em>Requirements</em></a> page for more details.</p></div></div><p>Spark SQL support is available under <code class="literal">org.elasticsearch.spark.sql</code> package.</p><h4><a id="spark-sql-write"></a>Writing <code class="literal">SchemaRDD</code> to Elasticsearch<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>With elasticsearch-hadoop, <code class="literal">SchemaRDD</code>s can be indexed to Elasticsearch.</p><p><strong>Scala. </strong>In Scala, simply import <code class="literal">org.elasticsearch.spark.sql</code> package which enriches the given <code class="literal">SchemaRDD</code> class with <code class="literal">saveToEs</code> methods; while these have the same signature as the <code class="literal">org.elasticsearch.spark</code> package, they are designed for <code class="literal">SchemaRDD</code> implementations:</p><pre class="programlisting prettyprint lang-scala">// reusing the example from Spark SQL documentation

import org.apache.spark.sql.SQLContext    <a id="CO61-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.apache.spark.sql.SQLContext._

import org.elasticsearch.spark.sql._      <a id="CO61-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>

...

// sc = existing SparkContext
val sqlContext = new SQLContext(sc)

// case class used to define the +RDD+ schema
case class Person(name: String, surname: String, age: Int)

//  create SchemaRDD
val people = sc.textFile("people.txt")    <a id="CO61-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
        .map(_.split(","))
        .map(p =&gt; Person(p(0), p(1), p(2).trim.toInt))


people.saveToEs("spark/people")           <a id="CO61-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO61-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark SQL package import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO61-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Spark package import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO61-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Read a text file as <span class="emphasis"><em>normal</em></span> <code class="literal">RDD</code> and map it to a <code class="literal">SchemaRDD</code> (using the <code class="literal">Person</code> case class)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO61-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Index the resulting <code class="literal">SchemaRDD</code> to Elasticsearch through the <code class="literal">saveToEs</code> method
</p></td></tr></table></div><p><strong>Java. </strong>In a similar fashion, for Java usage the dedicated package <code class="literal">org.elasticsearch.spark.sql.java.api</code> provides similar functionality through the <code class="literal">JavaEsSparkSQL</code> :</p><pre class="programlisting prettyprint lang-java">import org.apache.spark.sql.api.java.*;                      <a id="CO62-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.elasticsearch.spark.sql.java.api.JavaEsSparkSQL;  <a id="CO62-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
...

JavaSchemaRDD people = ...
JavaEsSparkSQL.saveToEs("spark/people");                     <a id="CO62-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark SQL Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop Spark SQL Java imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO62-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
index the <code class="literal">JavaSchemaRDD</code> in Elasticsearch under <code class="literal">spark/people</code>
</p></td></tr></table></div><p>Again, with Java 5 <span class="emphasis"><em>static</em></span> imports this can be further simplied to:</p><pre class="programlisting prettyprint lang-java">import static org.elasticsearch.spark.sql.java.api.JavaEsSparkSQL; <a id="CO63-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
...
saveToEs("spark/people");                                          <a id="CO63-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO63-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
statically import <code class="literal">JavaEsSparkSQL</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO63-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
call <code class="literal">saveToEs</code> method without having to type <code class="literal">JavaEsSpark</code> again
</p></td></tr></table></div><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>For maximum control over the mapping of your <code class="literal">SchemaRDD</code> in Elasticsearch, it is highly recommended to create the mapping before hand. See <a class="link" href="mapping.html" title="Mapping and Types">this</a> chapter for more information.</p></div></div><h4><a id="spark-sql-json"></a>Writing existing JSON to Elasticsearch<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>When using Spark SQL, if the input data is in JSON format, simply convert it to <code class="literal">SchemaRDD</code> (as described in Spark <a class="ulink" href="https://spark.apache.org/docs/latest/sql-programming-guide.html#json-datasets" target="_top">documentation</a>) through <code class="literal">SQLContext</code>/<code class="literal">JavaSQLContext</code> <code class="literal">jsonFile</code> methods.</p><h4><a id="spark-sql-read"></a>Using pure SQL to read from Elasticsearch<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><div class="important admon"><div class="icon"><img alt="Important" src="images/icons/important.png" /></div><div class="admon_content"><p>Available in Apache SparkSQL 1.2 (or higher)
IMPORTANT: The index and its mapping, have to exist prior to creating the temporary table</p></div></div><p>SparkSQL 1.2 <a class="ulink" href="http://spark.apache.org/releases/spark-release-1-2-0.html" target="_top">introduced</a> a new <a class="ulink" href="https://github.com/apache/spark/pull/2475" target="_top">API</a> for reading from external data sources, which is supported by elasticsearch-hadoop
simplifying the SQL configured needed for interacting with Elasticsearch. Further more, behind the scenes it understands the operations executed by Spark and thus can optimize the data and queries made (such as filtering or pruning),
improving performance.</p><p>To use it, declare a Spark temporary table backed by elasticsearch-hadoop:</p><pre class="programlisting prettyprint lang-scala">sqlContext.sql(
   "CREATE TEMPORARY TABLE myIndex    " + <a id="CO64-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
   "USING org.elasticsearch.spark.sql " + <a id="CO64-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
   "OPTIONS (resource 'spark/index')  " ) <a id="CO64-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO64-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark’s temporary table name
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO64-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
<code class="literal">USING</code> clause identifying the data source provider, in this case <code class="literal">org.elasticsearch.spark.sql</code>
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO64-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop <a class="link" href="configuration.html" title="Configuration">configuration options</a>, the mandatory one being <code class="literal">resource</code>. One can use the <code class="literal">es.</code> prefix or skip it for convenient.
</p></td></tr></table></div><p>Once defined, the schema is picked up automatically. So one can issue queries, right away:</p><pre class="programlisting prettyprint lang-sql">val allRDD = sqlContext.sql("SELECT * FROM myIndex WHERE id &lt;= 10")</pre><p>As elasticsearch-hadoop is aware of the queries being made, it can <span class="emphasis"><em>optimize</em></span> the requests done to Elasticsearch. For example, given the following query:</p><pre class="programlisting prettyprint lang-sql">val nameRDD = sqlContext.sql("SELECT name FROM myIndex WHERE id &gt;=1 AND id &lt;= 10")</pre><p>it knows only the <code class="literal">name</code> and <code class="literal">id</code> fields are required (the first to be returned to the user, the second for Spark’s internal filtering) and thus will ask <span class="emphasis"><em>only</em></span> for this data, making the queries quite efficient.</p><h4><a id="_reading_literal_schemardd_literal_s_from_elasticsearch"></a>Reading <code class="literal">SchemaRDD</code>s from Elasticsearch<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>As you might have guessed, one can define a <code class="literal">SchemaRDD</code> backed by Elasticsearch documents. Or even better, have them backed by a query result, effectively creating dynamic, real-time <span class="emphasis"><em>views</em></span> over your data.</p><p><strong>Scala. </strong>Through the <code class="literal">org.elasticsearch.spark.sql</code> package, <code class="literal">esRDD</code> methods are available on the <code class="literal">SQLContext</code> API:</p><pre class="programlisting prettyprint lang-scala">import org.apache.spark.sql.SQLContext        <a id="CO65-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

import org.elasticsearch.spark.sql._          <a id="CO65-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
...

val sql = new SQLContext(sc)

val people = sqlContext.esRDD("spark/people") <a id="CO65-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

// check the associated schema
println(people.schema)                        <a id="CO65-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
// root
//  |-- name: string (nullable = true)
//  |-- surname: string (nullable = true)
//  |-- age: long (nullable = true)           <a id="CO65-5"></a><span><img src="images/icons/callouts/5.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO65-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark SQL Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO65-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop SQL Scala imports
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO65-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
create a <code class="literal">SchemaRDD</code> backed by the <code class="literal">spark/people</code> index in Elasticsearch
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO65-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
the <code class="literal">SchemaRDD</code> associated schema discovered from Elasticsearch
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO65-5"><span><img src="images/icons/callouts/5.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
notice how the <code class="literal">age</code> field was transformed into a <code class="literal">Long</code> when using the default Elasticsearch mapping as discussed in the <a class="xref" href="mapping.html" title="Mapping and Types"><em>Mapping and Types</em></a> chapter.
</p></td></tr></table></div><p>And just as with the Spark <span class="emphasis"><em>core</em></span> support, additional parameters can be specified such as a query. This is quite a <span class="emphasis"><em>powerful</em></span> concept as one can filter the data at the source (Elasticsearch) and use Spark only on the results:</p><pre class="programlisting prettyprint lang-scala">// get only the Smiths
val smiths = sqlContext.esRDD("spark/people","?q=Smith" <a id="CO66-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>)</pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO66-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Elasticsearch query whose results comprise the <code class="literal">RDD</code>
</p></td></tr></table></div><p><strong>Java. </strong>
</p><pre class="programlisting prettyprint lang-java">import org.apache.spark.sql.api.java.JavaSQLContext;          <a id="CO67-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
import org.elasticsearch.spark.sql.java.api.JavaEsSparkSQL;   <a id="CO67-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
...
JavaSQLContext sql = new JavaSQLContext(sc);

JavaSchemaRDD people = JavaSQLContext.esRDD("spark/people");  <a id="CO67-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span></pre><p>
</p><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO67-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Spark SQL import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO67-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
elasticsearch-hadoop import
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO67-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
create a Java <code class="literal">SchemaRDD</code> backed by an Elasticsearch index
</p></td></tr></table></div><p>Better yet, the <code class="literal">JavaSchemaRDD</code> can be backed by a query result:</p><pre class="programlisting prettyprint lang-java">JavaSchemaRDD people = JavaSQLContext.esRDD("spark/people", "?q=Smith"  <a id="CO68-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>);</pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO68-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Elasticsearch query backing the elasticsearch-hadoop <code class="literal">SchemaRDD</code>
</p></td></tr></table></div><h3><a id="spark-mr"></a>Using the Map/Reduce layer<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h3><p>Another way of using Spark with Elasticsearch is through the Map/Reduce layer, that is by leveraging the dedicate <code class="literal">Input/OuputFormat</code> in elasticsearch-hadoop. However, unless one is stuck on
elasticsearch-hadoop 2.0, we <span class="emphasis"><em>strongly</em></span> recommend using the native integration as it offers significantly better performance and flexibility.</p><h4><a id="_configuration_3"></a>Configuration<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>Through elasticsearch-hadoop, Spark can integrate with Elasticsearch through its dedicated <code class="literal">InputFormat</code>, and in case of writing, through <code class="literal">OutputFormat</code>. These are described at length in the <a class="link" href="mapreduce.html" title="Map/Reduce integration">Map/Reduce</a> chapter so please refer to that for an in-depth explanation.</p><p>In short, one needs to setup a basic Hadoop <code class="literal">Configuration</code> object with the target Elasticsearch cluster and index, potentially a query, and she’s good to go.</p><p>From Spark’s perspective, the only thing required is setting up serialization - Spark relies by default on Java serialization which is convenient but fairly inefficient. This is the reason why Hadoop itself introduced its own serialization mechanism and its own types - namely <code class="literal">Writable</code>s. As such, <code class="literal">InputFormat</code> and <code class="literal">OutputFormat</code>s are required to return <code class="literal">Writables</code> which, out of the box, Spark does not understand.
The good news is, one can easily enable a different serialization (<a class="ulink" href="https://github.com/EsotericSoftware/kryo" target="_top">Kryo</a>) which handles the conversion automatically and also does this quite efficiently.</p><pre class="programlisting prettyprint lang-java">SparkConf sc = new SparkConf(); //.setMaster("local");
sc.set("spark.serializer", KryoSerializer.class.getName()); <a id="CO69-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>

// needed only when using the Java API
JavaSparkContext jsc = new JavaSparkContext(sc);</pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO69-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Enable the Kryo serialization support with Spark
</p></td></tr></table></div><p>Or if you prefer Scala</p><pre class="programlisting prettyprint lang-scala">val sc = new SparkConf(...)
sc.set("spark.serializer", classOf[KryoSerializer].getName) <a id="CO70-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span></pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO70-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Enable the Kryo serialization support with Spark
</p></td></tr></table></div><p>Note that the Kryo serialization is used as a work-around for dealing with <code class="literal">Writable</code> types; one can choose to convert the types directly (from <code class="literal">Writable</code> to <code class="literal">Serializable</code> types) - which is fine however for getting started, the one liner above seems to be the most effective.</p><h4><a id="_reading_data_from_elasticsearch_5"></a>Reading data from Elasticsearch<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h4><p>To read data, simply pass in the <code class="literal">org.elasticsearch.hadoop.mr.EsInputFormat</code> class - since it supports both the <code class="literal">old</code> and the <code class="literal">new</code> Map/Reduce APIs, you are free to use either method on <code class="literal">SparkContext</code>'s, <code class="literal">hadoopRDD</code> (which we recommend for conciseness reasons) or <code class="literal">newAPIHadoopRDD</code>. Which ever you chose, stick with it to avoid confusion and problems down the road.</p><h5><a id="_emphasis_old_emphasis_literal_org_apache_hadoop_mapred_literal_api_3"></a><span class="emphasis"><em>Old</em></span> (<code class="literal">org.apache.hadoop.mapred</code>) API<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><pre class="programlisting prettyprint lang-java">JobConf conf = new JobConf();                             <a id="CO71-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
conf.set("es.resource", "radio/artists");                 <a id="CO71-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
conf.set("es.query", "?q=me*");                           <a id="CO71-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

JavaPairRDD esRDD = jsc.hadoopRDD(conf, EsInputFormat.class,
                          Text.class, MapWritable.class); <a id="CO71-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
long docCount = esRDD.count();</pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create the Hadoop object (use the old API)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Configure the source (index)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Setup the query (optional)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO71-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a Spark <code class="literal">RDD</code> on top of Elasticsearch through <code class="literal">EsInputFormat</code> - the key represent the doc id, the value the doc itself
</p></td></tr></table></div><p>The Scala version is below:</p><pre class="programlisting prettyprint lang-scala">val conf = new JobConf()                                   <a id="CO72-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
conf.set("es.resource", "radio/artists")                   <a id="CO72-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
conf.set("es.query", "?q=me*")                             <a id="CO72-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
val esRDD = sc.hadoopRDD(conf,
                classOf[EsInputFormat[Text, MapWritable]], <a id="CO72-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
                classOf[Text], classOf[MapWritable]))
val docCount = esRDD.count();</pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO72-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create the Hadoop object (use the old API)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO72-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Configure the source (index)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO72-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Setup the query (optional)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO72-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a Spark <code class="literal">RDD</code> on top of Elasticsearch through <code class="literal">EsInputFormat</code>
</p></td></tr></table></div><h5><a id="_emphasis_new_emphasis_literal_org_apache_hadoop_mapreduce_literal_api_3"></a><span class="emphasis"><em>New</em></span> (<code class="literal">org.apache.hadoop.mapreduce</code>) API<a href="https://github.com/elasticsearch/elasticsearch-hadoop/edit/master/docs/src/reference/asciidoc/core/spark.adoc" class="edit_me" title="Edit this page on GitHub" rel="nofollow">edit</a></h5><p>As expected, the <code class="literal">mapreduce</code> API version is strikingly similar - replace <code class="literal">hadoopRDD</code> with <code class="literal">newAPIHadoopRDD</code> and <code class="literal">JobConf</code> with <code class="literal">Configuration</code>. That’s about it.</p><pre class="programlisting prettyprint lang-java">Configuration conf = new Configuration();       <a id="CO73-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
conf.set("es.resource", "radio/artists");       <a id="CO73-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
conf.set("es.query", "?q=me*");                 <a id="CO73-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>

JavaPairRDD esRDD = jsc.newAPIHadoopRDD(conf, EsInputFormat.class,
                Text.class, MapWritable.class); <a id="CO73-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
long docCount = esRDD.count();</pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO73-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create the Hadoop object (use the new API)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO73-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Configure the source (index)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO73-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Setup the query (optional)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO73-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a Spark <code class="literal">RDD</code> on top of Elasticsearch through <code class="literal">EsInputFormat</code> - the key represent the doc id, the value the doc itself
</p></td></tr></table></div><p>The Scala version is below:</p><pre class="programlisting prettyprint lang-scala">val conf = new Configuration()                             <a id="CO74-1"></a><span><img src="images/icons/callouts/1.png" alt="" /></span>
conf.set("es.resource", "radio/artists")                   <a id="CO74-2"></a><span><img src="images/icons/callouts/2.png" alt="" /></span>
conf.set("es.query", "?q=me*")                             <a id="CO74-3"></a><span><img src="images/icons/callouts/3.png" alt="" /></span>
val esRDD = sc.newHadoopRDD(conf,
                classOf[EsInputFormat[Text, MapWritable]], <a id="CO74-4"></a><span><img src="images/icons/callouts/4.png" alt="" /></span>
                classOf[Text], classOf[MapWritable]))
val docCount = esRDD.count();</pre><div class="calloutlist"><table border="0" summary="Callout list"><tr><td width="5%" valign="top" align="left"><p><a href="#CO74-1"><span><img src="images/icons/callouts/1.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create the Hadoop object (use the new API)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO74-2"><span><img src="images/icons/callouts/2.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Configure the source (index)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO74-3"><span><img src="images/icons/callouts/3.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Setup the query (optional)
</p></td></tr><tr><td width="5%" valign="top" align="left"><p><a href="#CO74-4"><span><img src="images/icons/callouts/4.png" alt="" /></span></a> </p></td><td valign="top" align="left"><p>
Create a Spark <code class="literal">RDD</code> on top of Elasticsearch through <code class="literal">EsInputFormat</code>
</p></td></tr></table></div></div><div class="navfooter"><span class="prev"><a href="pig.html">
              « 
              Apache Pig support</a>
           
        </span><span class="next">
           
          <a href="storm.html">Apache Storm support
               »
            </a></span></div>
<!-- end body -->

            </section>

            <!-- Footer -->
        
	<div id="footer-wrapper">
		<section id="footer" class="container">
			<div class="row">
				<div class="12u">
					<!-- Copyright -->
					<div id="copyright">
	<p>
		© 2015. All Rights Reserved - Elasticsearch
	</p>
	<ul class="links">
		<li>Elasticsearch is a trademark of Elasticsearch BV, registered in the U.S. and in other countries</li>
		<li><a href="/legal/trademarks" id="footer_trademarks">Trademarks</a></li>
		<li><a href="/legal/terms-of-use" id="footer_terms">Terms</a></li>
		<li><a href="/legal/privacy-policy" id="footer_privacy">Privacy</a></li>
	</ul>
	<p>
		Apache, Apache Lucene, Apache Hadoop, Hadoop, HDFS and the yellow elephant logo are trademarks of the <a href="http://www.apache.org/" target="_blank">Apache Software Foundation</a> in the United States and/or other&nbsp;countries.
	</p>
</div><!--<div class="row">
{replace4}
</div>-->
				</div>
			</div>					
		</section>
	</div>
			
	<!-- Social Media -->
	<div id="socialmedia-wrapper">
		<section id="socialmedia" class="container">
			<!-- social icons -->
			<div id="social">
				<ul class="links">
					
						<li class="facebook grayscale"><a href="http://www.facebook.com/elastic.co" target="_blank" id="footer_facebook"></a></li>
					
						<li class="twitter grayscale"><a href="https://www.twitter.com/elastic" target="_blank" id="footer_twitter"></a></li>
					
						<li class="linkedin grayscale"><a href="http://www.linkedin.com/company/elasticsearch" target="_blank" id="footer_linkedin"></a></li>
					
						<li class="xing grayscale"><a href="https://www.xing.com/companies/elastic.co" target="_blank" id="footer_xing"></a></li>
					
				</ul>
			</div>
		</section>
	</div>


<style type="text/css">
	
		.facebook{background:url("/assets/blte0d4d10ae3bd8337/social-media.png") no-repeat;}		
	
		.twitter{background:url("/assets/blte0d4d10ae3bd8337/social-media.png") no-repeat;}		
	
		.linkedin{background:url("/assets/blte0d4d10ae3bd8337/social-media.png") no-repeat;}		
	
		.xing{background:url("/assets/blte0d4d10ae3bd8337/social-media.png") no-repeat;}		
	
</style>	

  	</div>
  	<!-- <div class="preloader">
    	<div class="status"></div>
	</div> -->
  	<style></style>
   	<script src="/static/js/jquery.min.js"></script>
<script src="/static/js/jquery.autocomplete.js"></script>
<script src="/static/js/script.js"></script>
   	
   		<!-- Google Tag Manager -->
	<script>dataLayer = [];</script><noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-58RLH5"
													  height="0" width="0" style="display:none;visibility:hidden"></iframe></noscript>
	<script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
			new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
			j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
			'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-58RLH5');</script>
	<!-- End Google Tag Manager -->

            <script type="text/javascript">
  jQuery(function() {
    jQuery('#guide a[id]').each(function() { this.href='#'+this.id });
    jQuery('pre.prettyprint').each(function() {
      var pre = jQuery(this);
      pre.css('position','absolute');
      pre.css('width','auto');
      var height = pre.innerHeight();
      pre.wrap('<div class="pre_wrapper" style="height:' + height + 'px"></div>');
    })
  });
</script>
<script type='text/javascript' src='https://google-code-prettify.googlecode.com/svn/loader/run_prettify.js'></script>
<script type="text/javascript" >
  (function ($, $a, $title, $list) {
    $a = $('[id^="js-api-method-index"]');
    if (!$a.size()) return;
    $('#guide').addClass('js-client-docs');
    $list = $a.siblings('.itemizedlist').detach();
    $title = $(document.createElement('h2')).text('api methods')
    $a.parent().remove();
    $('.toc').first().append($(document.createElement('div')).addClass('js-api-method-index').append($title).append($list));
  }(jQuery));
</script>

            </body>
        